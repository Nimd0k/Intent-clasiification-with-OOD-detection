{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37150639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Раскоментить если вызапускаете код впервые: СЧИТАЕТ ЧАСА 2!!!\n",
    "\n",
    "# Загружаем дата-сет банкинг-77\n",
    "dataset = datasets.load_dataset('banking77')\n",
    "\n",
    "# Загружаем токенайзер и берт\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Токенизируем текст в данных (энкодим)\n",
    "train_encodings = tokenizer(dataset['train']['text'], truncation=True, padding=True)\n",
    "validation_encodings = tokenizer(dataset['test']['text'], truncation=True, padding=True)\n",
    "\n",
    "# Создаем эмбединги для тренировочной части данных\n",
    "train_embeddings = []\n",
    "for i in tqdm(range(len(dataset['train']['text']))):\n",
    "    input_ids = torch.tensor(train_encodings['input_ids'][i]).unsqueeze(0)\n",
    "    attention_mask = torch.tensor(train_encodings['attention_mask'][i]).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "    embeddings = output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    train_embeddings.append(embeddings)\n",
    "train_embeddings = np.array(train_embeddings)\n",
    "\n",
    "# То же самое, но для валидационной части\n",
    "validation_embeddings = []\n",
    "for i in tqdm(range(len(dataset['test']['text']))):\n",
    "    input_ids = torch.tensor(validation_encodings['input_ids'][i]).unsqueeze(0)\n",
    "    attention_mask = torch.tensor(validation_encodings['attention_mask'][i]).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask=attention_mask)\n",
    "    embeddings = output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    validation_embeddings.append(embeddings)\n",
    "validation_embeddings = np.array(validation_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = dataset['train']['label']\n",
    "# Группируем обучающие вложения(дальше эмбединги) с помощью K-средних (кнн)\n",
    "num_classes = len(np.unique(train_labels))\n",
    "kmeans = KMeans(n_clusters=num_classes, random_state=42)\n",
    "kmeans.fit(train_embeddings)\n",
    "train_distances = pairwise_distances(train_embeddings, kmeans.cluster_centers_)\n",
    "threshold = train_distances.max(axis=1).mean()\n",
    "\n",
    "# Создаем сет валидации для нахождения OOD (out of domain)\n",
    "validation_distances = pairwise_distances(validation_embeddings, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811cee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем тренировочные данные для out-of-domain классификатора\n",
    "ood_train_features = np.concatenate([train_embeddings[train_distances.max(axis=1) > threshold], train_embeddings], axis=0)\n",
    "ood_train_labels = ['in_domain'] * len(train_embeddings[train_distances.max(axis=1) > threshold]) + ['out_of_domain'] * len(train_embeddings)\n",
    "\n",
    "# Подставляем в наш классификатор данные (в нашем случае в Логистическую Регрессию) i.e.(обучаем классификатор)\n",
    "ood_classifier = LogisticRegression(random_state=42)\n",
    "ood_classifier.fit(ood_train_features, ood_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc75843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размечаем данные сета валидации используя OOD детекшн (находим грубо говоря, что лежит очень далеко от центра и кидаем бан в виде лейбла out_of_domain)\n",
    "validation_labels_pred = []\n",
    "for i in range(len(validation_embeddings)):\n",
    "    if validation_distances[i].min() > threshold:\n",
    "        validation_labels_pred.append('out_of_domain')\n",
    "    else:\n",
    "        validation_logits = model(torch.tensor(validation_encodings['input_ids'][i]).unsqueeze(0), attention_mask=torch.tensor(validation_encodings['attention_mask'][i]).unsqueeze(0))[0]\n",
    "        if ood_classifier.predict_proba(validation_embeddings[i].reshape(1, -1))[0][1] > 0.5:\n",
    "            validation_labels_pred.append('out_of_domain')\n",
    "        else:\n",
    "            validation_labels_pred.append(dataset['test'][i]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_labels_pred = [-1 if x == 'out_of_domain' else x for x in validation_labels_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b34aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрим перфоманс обученной на этот момент модели\n",
    "print(\"Accuracy with out-of-domain detection:\", accuracy_score(dataset['test']['label'], validation_labels_pred))\n",
    "print(\"F1 score with out-of-domain detection:\", f1_score(dataset['test']['label'], validation_labels_pred, average='weighted'))\n",
    "\n",
    "# Вычисляем расстояния между обучающими вложениями и центроидами кластеров\n",
    "train_distances = pairwise_distances(train_embeddings, kmeans.cluster_centers_)\n",
    "\n",
    "# Еще раз определяем трейнинг сет чтобы дальше его допилить с ин домэйн сэмплами\n",
    "ood_train_features = train_embeddings[train_distances.max(axis=1) > threshold]\n",
    "ood_train_labels = ['out_of_domain'] * len(ood_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff783d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем немного in-domain экземпляров в ood_train_features и ood_train_labels чтобы создать сбалансированный тренировочный сет\n",
    "num_in_domain_samples = min(len(train_embeddings), len(ood_train_features))\n",
    "idx = np.random.choice(len(train_embeddings), num_in_domain_samples, replace=False)\n",
    "in_domain_samples = train_embeddings[idx]\n",
    "ood_train_features = np.concatenate([ood_train_features, in_domain_samples])\n",
    "ood_train_labels = np.concatenate([ood_train_labels, ['in_domain'] * num_in_domain_samples])\n",
    "\n",
    "# Еще раз обучаем на новых данных, смотрим где результаты лучше(в тот или этот раз)\n",
    "ood_classifier = LogisticRegression(random_state=42)\n",
    "ood_classifier.fit(ood_train_features, ood_train_labels)\n",
    "\n",
    "# Тестируем out-of-domain классификатор\n",
    "ood_test_features = np.concatenate([train_embeddings[train_distances.max(axis=1) <= threshold], validation_embeddings])\n",
    "ood_test_labels = ['in_domain'] * len(train_embeddings[train_distances.max(axis=1) <= threshold]) + ['out_of_domain'] * len(validation_embeddings)\n",
    "ood_test_labels_pred = ood_classifier.predict(ood_test_features)\n",
    "print(\"Classification report for out-of-domain detection:\")\n",
    "print(classification_report(ood_test_labels, ood_test_labels_pred))\n",
    "\n",
    "# Получаем новый ф1 и аккураси скор (и немного других метрик)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a45580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем модель\n",
    "model_path = \"/path/to/model.pt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90ef4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем модель\n",
    "model_path = \"/path/to/model.pt\"\n",
    "state_dict = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ab934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Прикручиваем логрег к нашему берту\n",
    "class SentenceClassifier:\n",
    "    def __init__(self, model_path, tokenizer_path, ood_model_path):\n",
    "        # Грузим берт и токинайзер\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "        # Грузим нашу ООД классификатор - линрег\n",
    "        self.ood_model = torch.load(ood_model_path)\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        # Токенизируем полученный вход\n",
    "        inputs = self.tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "        # пропускаем через берта (нашего нафайнтьюненного) наше входное предложение\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        # Получаем предсказаный лейбл для нашего предложения и его вероятность отношения к ин домейн или наоборот оод\n",
    "        predicted_label = torch.argmax(outputs[0]).item()\n",
    "        in_domain_score = torch.softmax(outputs[0], dim=1)[0][predicted_label].item()\n",
    "\n",
    "        # пропускаем через оод классификатор\n",
    "        ood_label = self.ood_model.predict(sentence)\n",
    "\n",
    "        # ну и ретерним результат\n",
    "        return {\"in_domain_label\": predicted_label, \n",
    "                \"in_domain_score\": in_domain_score, \n",
    "                \"ood_label\": ood_label} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# указываем путь от нашей заранее сохраненной модели (в нашем случае путь до нафайнтьюненного берта) и загружаем ее\n",
    "model_path = \"path/to/fine-tuned/bert/model\"\n",
    "# тоукнайзер у нас стандартный\n",
    "tokenizer_path = \"bert-base-uncased\"\n",
    "# указываем путь от нашей заранее сохраненной модели (теперь от лин-регрессии) и загружаем ее\n",
    "ood_model_path = \"path/to/ood_classifier/model\"\n",
    "# созаем экземпляр класса и передаем данные о наших моделях\n",
    "classifier = SentenceClassifier(model_path, tokenizer_path, ood_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadaee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наше предложение на вход\n",
    "sentence = \"I want to open a checking account\"\n",
    "# отдельная переменная для предсказания о экземпляре класса given the input\n",
    "prediction = classifier.predict(sentence)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
